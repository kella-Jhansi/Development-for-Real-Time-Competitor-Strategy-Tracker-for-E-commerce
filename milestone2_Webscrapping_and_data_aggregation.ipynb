{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5B1SmXsYBax",
        "outputId": "ec9178c0-22d6-4462-d1c5-8606d48c61eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting playwright\n",
            "  Downloading playwright-1.57.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Collecting pyee<14,>=13 (from playwright)\n",
            "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from playwright) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from pyee<14,>=13->playwright) (4.15.0)\n",
            "Downloading playwright-1.57.0-py3-none-manylinux1_x86_64.whl (46.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-13.0.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyee, playwright\n",
            "Successfully installed playwright-1.57.0 pyee-13.0.0\n",
            "Downloading Chromium 143.0.7499.4 (playwright build v1200)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1200/chromium-linux.zip\u001b[22m\n",
            "(node:9420) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n",
            "(Use `node --trace-deprecation ...` to show where the warning was created)\n",
            "\u001b[1G164.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 0% 19.9s\u001b[0K\u001b[1G164.7 MiB [] 0% 14.5s\u001b[0K\u001b[1G164.7 MiB [] 0% 9.8s\u001b[0K\u001b[1G164.7 MiB [] 0% 8.6s\u001b[0K\u001b[1G164.7 MiB [] 1% 7.6s\u001b[0K\u001b[1G164.7 MiB [] 1% 7.3s\u001b[0K\u001b[1G164.7 MiB [] 1% 6.9s\u001b[0K\u001b[1G164.7 MiB [] 2% 6.2s\u001b[0K\u001b[1G164.7 MiB [] 2% 5.7s\u001b[0K\u001b[1G164.7 MiB [] 3% 5.3s\u001b[0K\u001b[1G164.7 MiB [] 3% 5.2s\u001b[0K\u001b[1G164.7 MiB [] 3% 5.0s\u001b[0K\u001b[1G164.7 MiB [] 4% 5.2s\u001b[0K\u001b[1G164.7 MiB [] 4% 5.5s\u001b[0K\u001b[1G164.7 MiB [] 5% 4.7s\u001b[0K\u001b[1G164.7 MiB [] 5% 4.5s\u001b[0K\u001b[1G164.7 MiB [] 6% 4.1s\u001b[0K\u001b[1G164.7 MiB [] 7% 3.9s\u001b[0K\u001b[1G164.7 MiB [] 7% 3.8s\u001b[0K\u001b[1G164.7 MiB [] 8% 3.8s\u001b[0K\u001b[1G164.7 MiB [] 8% 3.9s\u001b[0K\u001b[1G164.7 MiB [] 9% 3.9s\u001b[0K\u001b[1G164.7 MiB [] 9% 4.0s\u001b[0K\u001b[1G164.7 MiB [] 10% 4.0s\u001b[0K\u001b[1G164.7 MiB [] 10% 3.9s\u001b[0K\u001b[1G164.7 MiB [] 11% 3.8s\u001b[0K\u001b[1G164.7 MiB [] 11% 3.7s\u001b[0K\u001b[1G164.7 MiB [] 12% 3.7s\u001b[0K\u001b[1G164.7 MiB [] 12% 3.6s\u001b[0K\u001b[1G164.7 MiB [] 12% 3.7s\u001b[0K\u001b[1G164.7 MiB [] 13% 3.6s\u001b[0K\u001b[1G164.7 MiB [] 14% 3.4s\u001b[0K\u001b[1G164.7 MiB [] 15% 3.3s\u001b[0K\u001b[1G164.7 MiB [] 16% 3.1s\u001b[0K\u001b[1G164.7 MiB [] 16% 3.2s\u001b[0K\u001b[1G164.7 MiB [] 17% 3.0s\u001b[0K\u001b[1G164.7 MiB [] 18% 2.9s\u001b[0K\u001b[1G164.7 MiB [] 19% 2.8s\u001b[0K\u001b[1G164.7 MiB [] 20% 2.8s\u001b[0K\u001b[1G164.7 MiB [] 21% 2.8s\u001b[0K\u001b[1G164.7 MiB [] 22% 2.7s\u001b[0K\u001b[1G164.7 MiB [] 23% 2.6s\u001b[0K\u001b[1G164.7 MiB [] 24% 2.6s\u001b[0K\u001b[1G164.7 MiB [] 25% 2.5s\u001b[0K\u001b[1G164.7 MiB [] 26% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 27% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 27% 2.3s\u001b[0K\u001b[1G164.7 MiB [] 28% 2.3s\u001b[0K\u001b[1G164.7 MiB [] 29% 2.3s\u001b[0K\u001b[1G164.7 MiB [] 30% 2.3s\u001b[0K\u001b[1G164.7 MiB [] 31% 2.3s\u001b[0K\u001b[1G164.7 MiB [] 31% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 32% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 33% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 34% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 35% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 35% 2.5s\u001b[0K\u001b[1G164.7 MiB [] 36% 2.5s\u001b[0K\u001b[1G164.7 MiB [] 36% 2.6s\u001b[0K\u001b[1G164.7 MiB [] 37% 2.6s\u001b[0K\u001b[1G164.7 MiB [] 37% 2.7s\u001b[0K\u001b[1G164.7 MiB [] 38% 2.7s\u001b[0K\u001b[1G164.7 MiB [] 38% 2.8s\u001b[0K\u001b[1G164.7 MiB [] 39% 2.8s\u001b[0K\u001b[1G164.7 MiB [] 39% 2.9s\u001b[0K\u001b[1G164.7 MiB [] 40% 2.9s\u001b[0K\u001b[1G164.7 MiB [] 41% 2.9s\u001b[0K\u001b[1G164.7 MiB [] 42% 2.9s\u001b[0K\u001b[1G164.7 MiB [] 43% 2.9s\u001b[0K\u001b[1G164.7 MiB [] 44% 2.8s\u001b[0K\u001b[1G164.7 MiB [] 45% 2.8s\u001b[0K\u001b[1G164.7 MiB [] 46% 2.7s\u001b[0K\u001b[1G164.7 MiB [] 47% 2.6s\u001b[0K\u001b[1G164.7 MiB [] 48% 2.6s\u001b[0K\u001b[1G164.7 MiB [] 49% 2.6s\u001b[0K\u001b[1G164.7 MiB [] 50% 2.6s\u001b[0K\u001b[1G164.7 MiB [] 51% 2.6s\u001b[0K\u001b[1G164.7 MiB [] 52% 2.6s\u001b[0K\u001b[1G164.7 MiB [] 53% 2.5s\u001b[0K\u001b[1G164.7 MiB [] 54% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 55% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 55% 2.3s\u001b[0K\u001b[1G164.7 MiB [] 56% 2.3s\u001b[0K\u001b[1G164.7 MiB [] 57% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 58% 2.1s\u001b[0K\u001b[1G164.7 MiB [] 59% 2.1s\u001b[0K\u001b[1G164.7 MiB [] 59% 2.0s\u001b[0K\u001b[1G164.7 MiB [] 60% 2.0s\u001b[0K\u001b[1G164.7 MiB [] 61% 1.9s\u001b[0K\u001b[1G164.7 MiB [] 62% 1.9s\u001b[0K\u001b[1G164.7 MiB [] 63% 1.8s\u001b[0K\u001b[1G164.7 MiB [] 64% 1.8s\u001b[0K\u001b[1G164.7 MiB [] 64% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 65% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 66% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 66% 1.6s\u001b[0K\u001b[1G164.7 MiB [] 67% 1.6s\u001b[0K\u001b[1G164.7 MiB [] 68% 1.5s\u001b[0K\u001b[1G164.7 MiB [] 69% 1.5s\u001b[0K\u001b[1G164.7 MiB [] 69% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 70% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 71% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 71% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 72% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 73% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 73% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 74% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 75% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 75% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 76% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 77% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 77% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 78% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 79% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 79% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 80% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 81% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 82% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 82% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 83% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 84% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 84% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 85% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 86% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 86% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 87% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 88% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 89% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 90% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 90% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 91% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 92% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 92% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 93% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 94% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 95% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 96% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 96% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 97% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 98% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 99% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 143.0.7499.4 (playwright build v1200) downloaded to /root/.cache/ms-playwright/chromium-1200\n",
            "Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n",
            "(node:9491) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n",
            "(Use `node --trace-deprecation ...` to show where the warning was created)\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 6% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 20% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 48% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 86% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n",
            "Downloading Chromium Headless Shell 143.0.7499.4 (playwright build v1200)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1200/chromium-headless-shell-linux.zip\u001b[22m\n",
            "(node:9506) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n",
            "(Use `node --trace-deprecation ...` to show where the warning was created)\n",
            "\u001b[1G109.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 0% 12.5s\u001b[0K\u001b[1G109.7 MiB [] 0% 8.6s\u001b[0K\u001b[1G109.7 MiB [] 0% 5.1s\u001b[0K\u001b[1G109.7 MiB [] 1% 3.9s\u001b[0K\u001b[1G109.7 MiB [] 2% 2.8s\u001b[0K\u001b[1G109.7 MiB [] 3% 2.8s\u001b[0K\u001b[1G109.7 MiB [] 4% 2.7s\u001b[0K\u001b[1G109.7 MiB [] 5% 2.3s\u001b[0K\u001b[1G109.7 MiB [] 6% 2.3s\u001b[0K\u001b[1G109.7 MiB [] 6% 2.5s\u001b[0K\u001b[1G109.7 MiB [] 7% 2.2s\u001b[0K\u001b[1G109.7 MiB [] 9% 1.9s\u001b[0K\u001b[1G109.7 MiB [] 11% 1.7s\u001b[0K\u001b[1G109.7 MiB [] 12% 1.6s\u001b[0K\u001b[1G109.7 MiB [] 14% 1.5s\u001b[0K\u001b[1G109.7 MiB [] 15% 1.4s\u001b[0K\u001b[1G109.7 MiB [] 16% 1.4s\u001b[0K\u001b[1G109.7 MiB [] 18% 1.3s\u001b[0K\u001b[1G109.7 MiB [] 19% 1.3s\u001b[0K\u001b[1G109.7 MiB [] 21% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 23% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 24% 1.1s\u001b[0K\u001b[1G109.7 MiB [] 26% 1.1s\u001b[0K\u001b[1G109.7 MiB [] 28% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 29% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 31% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 32% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 33% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 35% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 36% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 38% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 40% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 42% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 43% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 44% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 46% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 48% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 49% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 52% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 53% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 55% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 57% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 58% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 59% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 60% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 61% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 63% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 65% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 66% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 68% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 70% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 71% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 73% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 74% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 76% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 78% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 79% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 81% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 83% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 84% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 85% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 87% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 89% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 91% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 92% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 94% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 96% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 98% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 143.0.7499.4 (playwright build v1200) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1200\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  at-spi2-core gsettings-desktop-schemas libatk1.0-data libxtst6\n",
            "  session-migration\n",
            "The following NEW packages will be installed:\n",
            "  at-spi2-core gsettings-desktop-schemas libatk-bridge2.0-0 libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libxcomposite1 libxtst6 session-migration\n",
            "0 upgraded, 9 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 318 kB of archives.\n",
            "After this operation, 1,497 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2,824 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
            "Fetched 318 kB in 1s (271 kB/s)\n",
            "Selecting previously unselected package libatspi2.0-0:amd64.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libatspi2.0-0_2.44.0-3_amd64.deb ...\n",
            "Unpacking libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../1-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package session-migration.\n",
            "Preparing to unpack .../2-session-migration_0.3.6_amd64.deb ...\n",
            "Unpacking session-migration (0.3.6) ...\n",
            "Selecting previously unselected package gsettings-desktop-schemas.\n",
            "Preparing to unpack .../3-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
            "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Selecting previously unselected package at-spi2-core.\n",
            "Preparing to unpack .../4-at-spi2-core_2.44.0-3_amd64.deb ...\n",
            "Unpacking at-spi2-core (2.44.0-3) ...\n",
            "Selecting previously unselected package libatk1.0-data.\n",
            "Preparing to unpack .../5-libatk1.0-data_2.36.0-3build1_all.deb ...\n",
            "Unpacking libatk1.0-data (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk1.0-0:amd64.\n",
            "Preparing to unpack .../6-libatk1.0-0_2.36.0-3build1_amd64.deb ...\n",
            "Unpacking libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk-bridge2.0-0:amd64.\n",
            "Preparing to unpack .../7-libatk-bridge2.0-0_2.38.0-3_amd64.deb ...\n",
            "Unpacking libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Selecting previously unselected package libxcomposite1:amd64.\n",
            "Preparing to unpack .../8-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
            "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up session-migration (0.3.6) ...\n",
            "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service â†’ /usr/lib/systemd/user/session-migration.service.\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Setting up libatk1.0-data (2.36.0-3build1) ...\n",
            "Setting up libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Setting up libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Setting up at-spi2-core (2.44.0-3) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install playwright nest_asyncio\n",
        "!playwright install chromium\n",
        "!apt-get install libatk1.0-0 libatk-bridge2.0-0 libatspi2.0-0 libxcomposite1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3VgS1rmYqzN",
        "outputId": "ee0c4b90-24ea-4e61-aad3-f997ed8fe1f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collected 6 products\n",
            "Saved CSV â†’ output/products_ajax.csv\n",
            "Saved JSON â†’ output/products_ajax.json\n"
          ]
        }
      ],
      "source": [
        "import asyncio, json, csv, time\n",
        "from pathlib import Path\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "#AJAX_URL = \"https://webscraper.io/test-sites/e-commerce/ajax\"  # target page\n",
        "#AJAX_URL = \"https://webscraper.io/test-sites\"\n",
        "AJAX_URL = \"https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=1\"\n",
        "async def scrape_ajax_site():\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)   # set False to watch it\n",
        "        ctx = await browser.new_context()\n",
        "        page = await ctx.new_page()\n",
        "        await page.goto(AJAX_URL, timeout=60000)\n",
        "\n",
        "        # Wait for initial product grid\n",
        "        await page.wait_for_selector(\".thumbnail\", timeout=30000)\n",
        "\n",
        "        # Keep clicking \"Load more\" until the button disappears or is disabled\n",
        "        while True:\n",
        "            try:\n",
        "                # Button sometimes labeled \"Load more\" or has class .btn-load-more\n",
        "                load_more = await page.query_selector(\"button:has-text('Load more')\")\n",
        "                if not load_more:\n",
        "                    break\n",
        "                is_disabled = await load_more.get_attribute(\"disabled\")\n",
        "                if is_disabled:\n",
        "                    break\n",
        "\n",
        "                # Click and wait for new items to appear\n",
        "                before_count = len(await page.query_selector_all(\".thumbnail\"))\n",
        "                await load_more.click()\n",
        "                # Wait for network idle and more thumbnails to load\n",
        "                await page.wait_for_load_state(\"networkidle\")\n",
        "                # Soft wait: poll until count increases or timeout\n",
        "                for _ in range(30):\n",
        "                    after_count = len(await page.query_selector_all(\".thumbnail\"))\n",
        "                    if after_count > before_count:\n",
        "                        break\n",
        "                    await asyncio.sleep(0.2)\n",
        "                # If no increase, assume end\n",
        "                if after_count <= before_count:\n",
        "                    break\n",
        "\n",
        "            except Exception:\n",
        "                # If the button isn't present or any error occurs, exit the loop\n",
        "                break\n",
        "\n",
        "        # Extract product cards\n",
        "        cards = await page.query_selector_all(\".thumbnail\")\n",
        "        rows = []\n",
        "\n",
        "        for card in cards:\n",
        "            # Title text and product link\n",
        "            title_el = await card.query_selector(\".title\")\n",
        "            title = (await title_el.text_content()).strip() if title_el else None\n",
        "            url = await title_el.get_attribute(\"href\") if title_el else None\n",
        "            # Price\n",
        "            price_el = await card.query_selector(\".price\")\n",
        "            price = (await price_el.text_content()).strip() if price_el else None\n",
        "            # Rating (count of .glyphicon-star)\n",
        "            stars = await card.query_selector_all(\".ratings .glyphicon-star\")\n",
        "            rating = len(stars) if stars else 0\n",
        "            # Image\n",
        "            img_el = await card.query_selector(\"img\")\n",
        "            img_src = await img_el.get_attribute(\"src\") if img_el else None\n",
        "\n",
        "            rows.append({\n",
        "                \"title\": title,\n",
        "                \"price\": price,\n",
        "                \"rating_stars\": rating,\n",
        "                \"product_url\": url,\n",
        "                \"image_url\": img_src\n",
        "            })\n",
        "\n",
        "        await browser.close()\n",
        "        return rows\n",
        "\n",
        "# Run and save\n",
        "data = asyncio.get_event_loop().run_until_complete(scrape_ajax_site())\n",
        "print(f\"Collected {len(data)} products\")\n",
        "\n",
        "# Save to CSV/JSON\n",
        "Path(\"output\").mkdir(exist_ok=True)\n",
        "csv_path = Path(\"output/products_ajax.csv\")\n",
        "json_path = Path(\"output/products_ajax.json\")\n",
        "\n",
        "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data)\n",
        "\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Saved CSV â†’ {csv_path}\")\n",
        "print(f\"Saved JSON â†’ {json_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RiU8imJY9wb",
        "outputId": "1f45f03f-345e-44c1-ef27-c1b1aa68fed8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” Collecting all category links...\n",
            "Found 2 categories\n",
            "\n",
            "ðŸ“‚ Scraping category: https://webscraper.io/test-sites/e-commerce/static/computers\n",
            "  ðŸ“ Subcategory: https://webscraper.io/test-sites/e-commerce/static/computers/laptops\n",
            "    âžœ Page 1 | 6 products\n",
            "    âžœ Page 2 | 6 products\n",
            "    âžœ Page 3 | 6 products\n",
            "    âžœ Page 4 | 6 products\n",
            "    âžœ Page 5 | 6 products\n",
            "    âžœ Page 6 | 6 products\n",
            "    âžœ Page 7 | 6 products\n",
            "    âžœ Page 8 | 6 products\n",
            "    âžœ Page 9 | 6 products\n",
            "    âžœ Page 10 | 6 products\n",
            "    âžœ Page 11 | 6 products\n",
            "    âžœ Page 12 | 6 products\n",
            "    âžœ Page 13 | 6 products\n",
            "    âžœ Page 14 | 6 products\n",
            "    âžœ Page 15 | 6 products\n",
            "    âžœ Page 16 | 6 products\n",
            "    âžœ Page 17 | 6 products\n",
            "    âžœ Page 18 | 6 products\n",
            "    âžœ Page 19 | 6 products\n",
            "    âžœ Page 20 | 3 products\n",
            "  ðŸ“ Subcategory: https://webscraper.io/test-sites/e-commerce/static/computers/tablets\n",
            "    âžœ Page 1 | 6 products\n",
            "    âžœ Page 2 | 6 products\n",
            "    âžœ Page 3 | 6 products\n",
            "    âžœ Page 4 | 3 products\n",
            "\n",
            "ðŸ“‚ Scraping category: https://webscraper.io/test-sites/e-commerce/static/phones\n",
            "  ðŸ“ Subcategory: https://webscraper.io/test-sites/e-commerce/static/phones/touch\n",
            "    âžœ Page 1 | 6 products\n",
            "    âžœ Page 2 | 3 products\n",
            "\n",
            "âœ… TOTAL PRODUCTS SCRAPED: 147\n",
            "\n",
            "ðŸ“ Saved CSV â†’ output/all_products.csv\n",
            "ðŸ“ Saved JSON â†’ output/all_products.json\n"
          ]
        }
      ],
      "source": [
        "import asyncio, json, csv\n",
        "from pathlib import Path\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "BASE = \"https://webscraper.io\"\n",
        "\n",
        "async def scrape_entire_website():\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        ctx = await browser.new_context()\n",
        "        page = await ctx.new_page()\n",
        "\n",
        "        print(\"ðŸ” Collecting all category links...\")\n",
        "        await page.goto(BASE + \"/test-sites/e-commerce/static\")\n",
        "\n",
        "        # Get all category URLs from left menu\n",
        "        category_links = await page.query_selector_all(\".category-link\")\n",
        "        categories = [BASE + await c.get_attribute(\"href\") for c in category_links]\n",
        "\n",
        "        print(f\"Found {len(categories)} categories\")\n",
        "\n",
        "        all_rows = []\n",
        "\n",
        "        # Loop through each category (Laptops, Phones, Tabletsâ€¦)\n",
        "        for cat_url in categories:\n",
        "            print(f\"\\nðŸ“‚ Scraping category: {cat_url}\")\n",
        "            await page.goto(cat_url)\n",
        "\n",
        "            # Subcategories inside each category\n",
        "            sub_links = await page.query_selector_all(\".subcategory-link\")\n",
        "            subcategories = [BASE + await s.get_attribute(\"href\") for s in sub_links]\n",
        "\n",
        "            if not subcategories:\n",
        "                # If no subcategory, treat category as product list\n",
        "                subcategories = [cat_url]\n",
        "\n",
        "            for sub_url in subcategories:\n",
        "                print(f\"  ðŸ“ Subcategory: {sub_url}\")\n",
        "\n",
        "                page_no = 1\n",
        "                while True:\n",
        "                    url = sub_url + f\"?page={page_no}\"\n",
        "                    await page.goto(url)\n",
        "                    await page.wait_for_load_state(\"networkidle\")\n",
        "\n",
        "                    cards = await page.query_selector_all(\".thumbnail\")\n",
        "                    if len(cards) == 0:\n",
        "                        break\n",
        "\n",
        "                    print(f\"    âžœ Page {page_no} | {len(cards)} products\")\n",
        "\n",
        "                    for card in cards:\n",
        "                        title_el = await card.query_selector(\".title\")\n",
        "                        title = (await title_el.text_content()).strip()\n",
        "                        product_url = BASE + await title_el.get_attribute(\"href\")\n",
        "\n",
        "                        price = (await (await card.query_selector(\".price\")).text_content()).strip()\n",
        "\n",
        "                        desc_el = await card.query_selector(\".description\")\n",
        "                        desc = (await desc_el.text_content()).strip() if desc_el else \"\"\n",
        "\n",
        "                        stars = await card.query_selector_all(\".ratings .glyphicon-star\")\n",
        "                        rating = len(stars)\n",
        "\n",
        "                        img_el = await card.query_selector(\"img\")\n",
        "                        img_src = BASE + await img_el.get_attribute(\"src\")\n",
        "\n",
        "                        all_rows.append({\n",
        "                            \"title\": title,\n",
        "                            \"price\": price,\n",
        "                            \"rating\": rating,\n",
        "                            \"description\": desc,\n",
        "                            \"product_url\": product_url,\n",
        "                            \"image_url\": img_src,\n",
        "                            \"category\": cat_url,\n",
        "                            \"subcategory\": sub_url,\n",
        "                            \"page\": page_no\n",
        "                        })\n",
        "\n",
        "                    page_no += 1\n",
        "\n",
        "        await browser.close()\n",
        "        return all_rows\n",
        "\n",
        "\n",
        "# Run scraper\n",
        "data = asyncio.get_event_loop().run_until_complete(scrape_entire_website())\n",
        "print(f\"\\nâœ… TOTAL PRODUCTS SCRAPED: {len(data)}\")\n",
        "\n",
        "# Save output\n",
        "Path(\"output\").mkdir(exist_ok=True)\n",
        "csv_path = Path(\"output/all_products.csv\")\n",
        "json_path = Path(\"output/all_products.json\")\n",
        "\n",
        "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data)\n",
        "\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\nðŸ“ Saved CSV â†’ {csv_path}\")\n",
        "print(f\"ðŸ“ Saved JSON â†’ {json_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2t5IwstkW6I",
        "outputId": "89a0e92e-e395-42b8-8f09-27d700c07215"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping category: Books ...\n",
            "Scraping category: Travel ...\n",
            "Scraping category: Mystery ...\n",
            "Scraping category: Historical Fiction ...\n",
            "Scraping category: Sequential Art ...\n",
            "Scraping category: Classics ...\n",
            "Scraping category: Philosophy ...\n",
            "Scraping category: Romance ...\n",
            "Scraping category: Womens Fiction ...\n",
            "Scraping category: Fiction ...\n",
            "Scraping category: Childrens ...\n",
            "Scraping category: Religion ...\n",
            "Scraping category: Nonfiction ...\n",
            "Scraping category: Music ...\n",
            "Scraping category: Default ...\n",
            "Scraping category: Science Fiction ...\n",
            "Scraping category: Sports and Games ...\n",
            "Scraping category: Add a comment ...\n",
            "Scraping category: Fantasy ...\n",
            "Scraping category: New Adult ...\n",
            "Scraping category: Young Adult ...\n",
            "Scraping category: Science ...\n",
            "Scraping category: Poetry ...\n",
            "Scraping category: Paranormal ...\n",
            "Scraping category: Art ...\n",
            "Scraping category: Psychology ...\n",
            "Scraping category: Autobiography ...\n",
            "Scraping category: Parenting ...\n",
            "Scraping category: Adult Fiction ...\n",
            "Scraping category: Humor ...\n",
            "Scraping category: Horror ...\n",
            "Scraping category: History ...\n",
            "Scraping category: Food and Drink ...\n",
            "Scraping category: Christian Fiction ...\n",
            "Scraping category: Business ...\n",
            "Scraping category: Biography ...\n",
            "Scraping category: Thriller ...\n",
            "Scraping category: Contemporary ...\n",
            "Scraping category: Spirituality ...\n",
            "Scraping category: Academic ...\n",
            "Scraping category: Self Help ...\n",
            "Scraping category: Historical ...\n",
            "Scraping category: Christian ...\n",
            "Scraping category: Suspense ...\n",
            "Scraping category: Short Stories ...\n",
            "Scraping category: Novels ...\n",
            "Scraping category: Health ...\n",
            "Scraping category: Politics ...\n",
            "Scraping category: Cultural ...\n",
            "Scraping category: Erotica ...\n",
            "Scraping category: Crime ...\n",
            "\n",
            "âš  All books are in stock â†’ removing discounts\n",
            "\n",
            "ðŸ“˜ SAMPLE DATA (Table with Borders):\n",
            "+----+------------+------------------------------------------------------------------------------------------------+---------+------------------+----------+\n",
            "|    | Category   | Title                                                                                          |   Price |   Adjusted Price |   Rating |\n",
            "+====+============+================================================================================================+=========+==================+==========+\n",
            "|  0 | Books      | A Light in the Attic                                                                           |   51.77 |            56.95 |        3 |\n",
            "+----+------------+------------------------------------------------------------------------------------------------+---------+------------------+----------+\n",
            "|  1 | Books      | Tipping the Velvet                                                                             |   53.74 |            59.11 |        1 |\n",
            "+----+------------+------------------------------------------------------------------------------------------------+---------+------------------+----------+\n",
            "|  2 | Books      | Soumission                                                                                     |   50.1  |            55.11 |        1 |\n",
            "+----+------------+------------------------------------------------------------------------------------------------+---------+------------------+----------+\n",
            "|  3 | Books      | Sharp Objects                                                                                  |   47.82 |            52.6  |        4 |\n",
            "+----+------------+------------------------------------------------------------------------------------------------+---------+------------------+----------+\n",
            "|  4 | Books      | Sapiens: A Brief History of Humankind                                                          |   54.23 |            59.65 |        5 |\n",
            "+----+------------+------------------------------------------------------------------------------------------------+---------+------------------+----------+\n",
            "|  5 | Books      | The Requiem Red                                                                                |   22.65 |            24.91 |        1 |\n",
            "+----+------------+------------------------------------------------------------------------------------------------+---------+------------------+----------+\n",
            "|  6 | Books      | The Dirty Little Secrets of Getting Your Dream Job                                             |   33.34 |            36.67 |        4 |\n",
            "+----+------------+------------------------------------------------------------------------------------------------+---------+------------------+----------+\n",
            "|  7 | Books      | The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull        |   17.93 |            19.72 |        3 |\n",
            "+----+------------+------------------------------------------------------------------------------------------------+---------+------------------+----------+\n",
            "|  8 | Books      | The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics |   22.6  |            24.86 |        4 |\n",
            "+----+------------+------------------------------------------------------------------------------------------------+---------+------------------+----------+\n",
            "|  9 | Books      | The Black Maria                                                                                |   52.15 |            57.37 |        1 |\n",
            "+----+------------+------------------------------------------------------------------------------------------------+---------+------------------+----------+\n",
            "\n",
            "ðŸ† HIGHEST RATED BOOK:\n",
            "+---------------------------------------+------------+----------+---------+\n",
            "| Title                                 | Category   |   Rating |   Price |\n",
            "+=======================================+============+==========+=========+\n",
            "| Sapiens: A Brief History of Humankind | Books      |        5 |   54.23 |\n",
            "+---------------------------------------+------------+----------+---------+\n",
            "\n",
            "ðŸ”» LOWEST RATED BOOK:\n",
            "+--------------------+------------+----------+---------+\n",
            "| Title              | Category   |   Rating |   Price |\n",
            "+====================+============+==========+=========+\n",
            "| Tipping the Velvet | Books      |        1 |   53.74 |\n",
            "+--------------------+------------+----------+---------+\n",
            "\n",
            "Saved: all_books_final_with_all_features.csv âœ”\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from tabulate import tabulate\n",
        "\n",
        "BASE_URL = \"https://books.toscrape.com/\"\n",
        "\n",
        "# Rating mapping\n",
        "RATING_MAP = {\n",
        "    \"One\": 1,\n",
        "    \"Two\": 2,\n",
        "    \"Three\": 3,\n",
        "    \"Four\": 4,\n",
        "    \"Five\": 5\n",
        "}\n",
        "\n",
        "def get_soup(url):\n",
        "    resp = requests.get(url)\n",
        "    resp.encoding = \"utf-8\"\n",
        "    return BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "def clean_price(price_text):\n",
        "    cleaned = \"\".join(c for c in price_text if c.isdigit() or c == \".\")\n",
        "    return float(cleaned)\n",
        "\n",
        "def get_categories():\n",
        "    soup = get_soup(BASE_URL)\n",
        "    cat_list = soup.find(\"ul\", class_=\"nav-list\").find(\"ul\")\n",
        "\n",
        "    categories = {\n",
        "        \"Books\": BASE_URL + \"catalogue/page-1.html\"\n",
        "    }\n",
        "\n",
        "    for li in cat_list.find_all(\"li\"):\n",
        "        name = li.text.strip()\n",
        "        link = BASE_URL + li.find(\"a\")[\"href\"]\n",
        "        categories[name] = link\n",
        "\n",
        "    return categories\n",
        "\n",
        "def scrape_category(category_name, category_url):\n",
        "    books = []\n",
        "\n",
        "    while True:\n",
        "        soup = get_soup(category_url)\n",
        "\n",
        "        for article in soup.find_all(\"article\", class_=\"product_pod\"):\n",
        "\n",
        "            title = article.h3.a[\"title\"]\n",
        "            price = clean_price(article.find(\"p\", class_=\"price_color\").text)\n",
        "            availability = article.find(\"p\", class_=\"instock availability\").text.strip()\n",
        "\n",
        "            rating_word = article.find(\"p\")[\"class\"][1]\n",
        "            rating = RATING_MAP.get(rating_word, 0)\n",
        "\n",
        "            product_href = article.h3.a[\"href\"].replace(\"../\", \"\")\n",
        "            product_url = BASE_URL + \"catalogue/\" + product_href\n",
        "            product_soup = get_soup(product_url)\n",
        "\n",
        "            description = \"No description\"\n",
        "            desc = product_soup.find(\"div\", id=\"product_description\")\n",
        "            if desc:\n",
        "                description = desc.find_next(\"p\").text.strip()\n",
        "\n",
        "            # Adjusted price (+10% if in stock)\n",
        "            adjusted_price = round(price * 1.10, 2) if \"In stock\" in availability else price\n",
        "\n",
        "            books.append({\n",
        "                \"Category\": category_name,\n",
        "                \"Title\": title,\n",
        "                \"Price\": price,\n",
        "                \"Availability\": availability,\n",
        "                \"Rating\": rating,\n",
        "                \"Adjusted Price\": adjusted_price,\n",
        "                \"Description\": description\n",
        "            })\n",
        "\n",
        "        next_btn = soup.find(\"li\", class_=\"next\")\n",
        "        if next_btn:\n",
        "            next_page = next_btn.a[\"href\"]\n",
        "\n",
        "            if \"catalogue\" not in category_url:\n",
        "                category_url = BASE_URL + \"catalogue/\" + next_page\n",
        "            else:\n",
        "                base = category_url.rsplit(\"/\", 1)[0]\n",
        "                category_url = f\"{base}/{next_page}\"\n",
        "\n",
        "            time.sleep(0.4)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return books\n",
        "\n",
        "def scrape_all_books():\n",
        "    categories = get_categories()\n",
        "    all_books = []\n",
        "\n",
        "    for cat_name, cat_url in categories.items():\n",
        "        print(f\"Scraping category: {cat_name} ...\")\n",
        "        books = scrape_category(cat_name, cat_url)\n",
        "        all_books.extend(books)\n",
        "\n",
        "    df = pd.DataFrame(all_books)\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # Apply Discount Rules\n",
        "    # ------------------------------------------------\n",
        "\n",
        "    # 50% discount for users who did not buy any book\n",
        "    df[\"Discounted Price (No Purchase)\"] = round(df[\"Price\"] * 0.50, 2)\n",
        "\n",
        "    # First book discount: 20%\n",
        "    df[\"First Book Discount Price\"] = round(df[\"Price\"] * 0.80, 2)\n",
        "\n",
        "    # If all books are in stock â†’ remove discounts\n",
        "    if all(\"In stock\" in a for a in df[\"Availability\"]):\n",
        "        print(\"\\nâš  All books are in stock â†’ removing discounts\")\n",
        "        df[\"Discounted Price (No Purchase)\"] = df[\"Price\"]\n",
        "        df[\"First Book Discount Price\"] = df[\"Price\"]\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # TABLE OUTPUT\n",
        "    # ------------------------------------------------\n",
        "    print(\"\\nðŸ“˜ SAMPLE DATA (Table with Borders):\")\n",
        "    table_preview = df.head(10)[[\"Category\", \"Title\", \"Price\", \"Adjusted Price\", \"Rating\"]]\n",
        "    print(tabulate(table_preview, headers=\"keys\", tablefmt=\"grid\"))\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # Highest Rating Book\n",
        "    # ------------------------------------------------\n",
        "    highest_rating = df[\"Rating\"].max()\n",
        "    best_book = df[df[\"Rating\"] == highest_rating].iloc[0]\n",
        "\n",
        "    print(\"\\nðŸ† HIGHEST RATED BOOK:\")\n",
        "    print(tabulate(\n",
        "        [[best_book[\"Title\"], best_book[\"Category\"], best_book[\"Rating\"], best_book[\"Price\"]]],\n",
        "        headers=[\"Title\", \"Category\", \"Rating\", \"Price\"],\n",
        "        tablefmt=\"grid\"\n",
        "    ))\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # Lowest Rating Book\n",
        "    # ------------------------------------------------\n",
        "    lowest_rating = df[\"Rating\"].min()\n",
        "    worst_book = df[df[\"Rating\"] == lowest_rating].iloc[0]\n",
        "\n",
        "    print(\"\\nðŸ”» LOWEST RATED BOOK:\")\n",
        "    print(tabulate(\n",
        "        [[worst_book[\"Title\"], worst_book[\"Category\"], worst_book[\"Rating\"], worst_book[\"Price\"]]],\n",
        "        headers=[\"Title\", \"Category\", \"Rating\", \"Price\"],\n",
        "        tablefmt=\"grid\"\n",
        "    ))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Run scraper\n",
        "df_books = scrape_all_books()\n",
        "\n",
        "df_books.to_csv(\"all_books_final_with_all_features.csv\", index=False)\n",
        "print(\"\\nSaved: all_books_final_with_all_features.csv âœ”\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RRyUxZIynKYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# -----------------------------\n",
        "# NLTK SETUP\n",
        "# -----------------------------\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "STOP_WORDS = stopwords.words(\"english\")\n",
        "\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "BOOKS_URL = \"https://books.toscrape.com/\"\n",
        "BBC_NEWS_URL = \"https://www.bbc.com/news\"\n",
        "\n",
        "RATING_MAP = {\n",
        "    \"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4, \"Five\": 5\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# STEP 1: SCRAPE BOOKS\n",
        "# =====================================================\n",
        "def scrape_all_books(max_books=1000):\n",
        "    books = []\n",
        "    page_url = BOOKS_URL\n",
        "\n",
        "    while page_url and len(books) < max_books:\n",
        "        soup = BeautifulSoup(\n",
        "            requests.get(page_url, headers=HEADERS).text,\n",
        "            \"html.parser\"\n",
        "        )\n",
        "\n",
        "        for article in soup.select(\"article.product_pod\"):\n",
        "            if len(books) >= max_books:\n",
        "                break\n",
        "\n",
        "            title = article.h3.a[\"title\"]\n",
        "\n",
        "            price = float(\n",
        "                re.findall(r\"\\d+\\.\\d+\", article.select_one(\".price_color\").text)[0]\n",
        "            )\n",
        "\n",
        "            rating_class = article.select_one(\"p.star-rating\")[\"class\"][1]\n",
        "            rating = RATING_MAP.get(rating_class, 0)\n",
        "\n",
        "            books.append({\n",
        "                \"title\": title,\n",
        "                \"price\": price,\n",
        "                \"rating\": rating\n",
        "            })\n",
        "\n",
        "        next_btn = soup.select_one(\"li.next a\")\n",
        "        page_url = BOOKS_URL + next_btn[\"href\"] if next_btn else None\n",
        "\n",
        "    return books\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# STEP 2: SCRAPE BBC NEWS\n",
        "# =====================================================\n",
        "def scrape_bbc_news(max_news=30):\n",
        "    soup = BeautifulSoup(\n",
        "        requests.get(BBC_NEWS_URL, headers=HEADERS).text,\n",
        "        \"html.parser\"\n",
        "    )\n",
        "\n",
        "    urls = []\n",
        "    for a in soup.select(\"a[href^='/news']\"):\n",
        "        url = \"https://www.bbc.com\" + a[\"href\"]\n",
        "        if url not in urls:\n",
        "            urls.append(url)\n",
        "        if len(urls) >= max_news:\n",
        "            break\n",
        "\n",
        "    news_data = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            page = BeautifulSoup(\n",
        "                requests.get(url, headers=HEADERS, timeout=5).text,\n",
        "                \"html.parser\"\n",
        "            )\n",
        "\n",
        "            paragraphs = [p.text.strip() for p in page.select(\"article p\")]\n",
        "            description = \" \".join(paragraphs)\n",
        "\n",
        "            if len(description) < 50:\n",
        "                continue\n",
        "\n",
        "            news_data.append({\n",
        "                \"url\": url,\n",
        "                \"description\": description\n",
        "            })\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return news_data\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# STEP 3 & 4: COSINE SIMILARITY + DISCOUNT\n",
        "# =====================================================\n",
        "def apply_discount_using_cosine(books, news, threshold=0.15):\n",
        "\n",
        "    # Combine all news text\n",
        "    news_text = \" \".join(n[\"description\"] for n in news)\n",
        "    news_text = re.sub(r\"[^a-zA-Z ]\", \"\", news_text).lower()\n",
        "\n",
        "    documents = [news_text] + [\n",
        "        re.sub(r\"[^a-zA-Z ]\", \"\", b[\"title\"]).lower()\n",
        "        for b in books\n",
        "    ]\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words=STOP_WORDS)\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "    news_vector = tfidf_matrix[0]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, book in enumerate(books):\n",
        "        book_vector = tfidf_matrix[i + 1]\n",
        "        similarity = cosine_similarity(news_vector, book_vector)[0][0]\n",
        "\n",
        "        matched = similarity >= threshold\n",
        "\n",
        "        discounted_price = (\n",
        "            round(book[\"price\"] * 0.50, 2) if matched else book[\"price\"]\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            \"Title\": book[\"title\"],\n",
        "            \"Rating\": book[\"rating\"],\n",
        "            \"Original Price\": book[\"price\"],\n",
        "            \"Discounted Price\": discounted_price,\n",
        "            \"Cosine Similarity\": round(similarity, 3),\n",
        "            \"Matched\": matched\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# STEP 5: SAVE CSV\n",
        "# =====================================================\n",
        "def save_to_csv(books):\n",
        "    pd.DataFrame(books).to_csv(\n",
        "        \"book_discount_based_on_news_cosine.csv\",\n",
        "        index=False\n",
        "    )\n",
        "    print(\"CSV saved as: book_discount_based_on_news_cosine.csv\")\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# MAIN\n",
        "# =====================================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Scraping books...\")\n",
        "    books = scrape_all_books()\n",
        "\n",
        "    print(\"Scraping news...\")\n",
        "    news = scrape_bbc_news()\n",
        "\n",
        "    print(\"Applying cosine similarity discount...\")\n",
        "    discounted_books = apply_discount_using_cosine(books, news)\n",
        "\n",
        "    save_to_csv(discounted_books)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBUk2nySmnCl",
        "outputId": "6237b028-96a3-45ea-9acb-f03230046e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping books...\n",
            "Scraping news...\n",
            "Applying cosine similarity discount...\n",
            "CSV saved as: book_discount_based_on_news_cosine.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "BASE_URL = \"https://books.toscrape.com/catalogue/\"\n",
        "PAGE_URL = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
        "BBC_NEWS_URL = \"https://www.bbc.com/news\"\n",
        "OUTPUT_FILE = \"books_with_news_discounts.csv\"\n",
        "TOTAL_PAGES = 50\n",
        "\n",
        "def get_bbc_keywords():\n",
        "    \"\"\"Fetches top 30 news URLs from BBC and extracts top keywords.\"\"\"\n",
        "    print(\"Fetching keywords from BBC News...\")\n",
        "    try:\n",
        "        response = requests.get(BBC_NEWS_URL, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find article links\n",
        "        links = []\n",
        "        for a in soup.find_all('a', href=True):\n",
        "            href = a['href']\n",
        "            if \"/news/\" in href and not href.endswith(\"/news\"):\n",
        "                full_url = \"https://www.bbc.com\" + href if href.startswith('/') else href\n",
        "                if full_url not in links:\n",
        "                    links.append(full_url)\n",
        "            if len(links) >= 30: break\n",
        "\n",
        "        all_words = []\n",
        "        stopwords = {'the', 'that', 'with', 'from', 'this', 'they', 'have', 'were', 'their', 'been', 'said', 'says', 'about'}\n",
        "\n",
        "        for link in links:\n",
        "            try:\n",
        "                res = requests.get(link, timeout=5)\n",
        "                article_soup = BeautifulSoup(res.text, 'html.parser')\n",
        "                text = article_soup.get_text().lower()\n",
        "                words = re.findall(r'\\b[a-z]{4,}\\b', text)\n",
        "                all_words.extend([w for w in words if w not in stopwords])\n",
        "            except: continue\n",
        "\n",
        "        # Return top 50 most common keywords across all 30 articles\n",
        "        return [word for word, count in Counter(all_words).most_common(50)]\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching news: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_rating_value(rating_class):\n",
        "    \"\"\"Converts star rating class (e.g., 'Three') to a number.\"\"\"\n",
        "    ratings = {\"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4, \"Five\": 5}\n",
        "    return ratings.get(rating_class, -1)\n",
        "\n",
        "def scrape_books_with_logic():\n",
        "    keywords = get_bbc_keywords()\n",
        "    print(f\"Top News Keywords: {keywords[:10]}...\")\n",
        "\n",
        "    with open(OUTPUT_FILE, mode='w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Title\", \"Original Price\", \"Final Price\", \"Discount\", \"Rating\", \"Stock\", \"Author\", \"Review Snippet\"])\n",
        "\n",
        "        for page_num in range(1, TOTAL_PAGES + 1):\n",
        "            print(f\"Processing Page {page_num}...\")\n",
        "            res = requests.get(PAGE_URL.format(page_num))\n",
        "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "            for pod in soup.find_all(\"article\", class_=\"product_pod\"):\n",
        "                title = pod.h3.a[\"title\"]\n",
        "\n",
        "                # Navigate to detail page for Stock and Description\n",
        "                detail_url = BASE_URL + pod.h3.a[\"href\"].replace(\"../../../\", \"\")\n",
        "                detail_res = requests.get(detail_url)\n",
        "                d_soup = BeautifulSoup(detail_res.text, \"html.parser\")\n",
        "\n",
        "                # Get Description & Price (Remove Â£ symbol)\n",
        "                desc = d_soup.select_one('article.product_page > p').text if d_soup.select_one('article.product_page > p') else \"\"\n",
        "                price_raw = pod.find(\"p\", class_=\"price_color\").text\n",
        "                price = float(re.sub(r'[^\\d.]', '', price_raw))\n",
        "\n",
        "                # Get Stock (Number)\n",
        "                stock_text = d_soup.find(\"p\", class_=\"instock availability\").text.strip()\n",
        "                stock_count = \"\".join(filter(str.isdigit, stock_text)) or \"0\"\n",
        "\n",
        "                # Get Rating\n",
        "                rating_class = pod.p['class'][1]\n",
        "                rating_val = get_rating_value(rating_class)\n",
        "\n",
        "                # DISCOUNT LOGIC\n",
        "                match = any(word in desc.lower() for word in keywords)\n",
        "                final_price = round(price * 0.5, 2) if match else price\n",
        "                discount_applied = \"50%\" if match else \"0%\"\n",
        "\n",
        "                # Write to CSV\n",
        "                writer.writerow([title, price, final_price, discount_applied, rating_val, stock_count, \"N/A\", desc[:150]])\n",
        "\n",
        "    print(f\"Success! Data saved to {OUTPUT_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    scrape_books_with_logic()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJVgPM84lNgb",
        "outputId": "03d4228b-16b7-410a-8715-7d956e690507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching keywords from BBC News...\n",
            "Top News Keywords: ['days', 'after', 'ukraine', 'gaza', 'will', 'russian', 'over', 'year', 'latest', 'trump']...\n",
            "Processing Page 1...\n",
            "Processing Page 2...\n",
            "Processing Page 3...\n",
            "Processing Page 4...\n",
            "Processing Page 5...\n",
            "Processing Page 6...\n",
            "Processing Page 7...\n",
            "Processing Page 8...\n",
            "Processing Page 9...\n",
            "Processing Page 10...\n",
            "Processing Page 11...\n",
            "Processing Page 12...\n",
            "Processing Page 13...\n",
            "Processing Page 14...\n",
            "Processing Page 15...\n",
            "Processing Page 16...\n",
            "Processing Page 17...\n",
            "Processing Page 18...\n",
            "Processing Page 19...\n",
            "Processing Page 20...\n",
            "Processing Page 21...\n",
            "Processing Page 22...\n",
            "Processing Page 23...\n",
            "Processing Page 24...\n",
            "Processing Page 25...\n",
            "Processing Page 26...\n",
            "Processing Page 27...\n",
            "Processing Page 28...\n",
            "Processing Page 29...\n",
            "Processing Page 30...\n",
            "Processing Page 31...\n",
            "Processing Page 32...\n",
            "Processing Page 33...\n",
            "Processing Page 34...\n",
            "Processing Page 35...\n",
            "Processing Page 36...\n",
            "Processing Page 37...\n",
            "Processing Page 38...\n",
            "Processing Page 39...\n",
            "Processing Page 40...\n",
            "Processing Page 41...\n",
            "Processing Page 42...\n",
            "Processing Page 43...\n",
            "Processing Page 44...\n",
            "Processing Page 45...\n",
            "Processing Page 46...\n",
            "Processing Page 47...\n",
            "Processing Page 48...\n",
            "Processing Page 49...\n",
            "Processing Page 50...\n",
            "Success! Data saved to books_with_news_discounts.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "BASE_URL = \"https://books.toscrape.com/catalogue/\"\n",
        "PAGE_URL = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
        "BBC_NEWS_URL = \"https://www.bbc.com/news\"\n",
        "OUTPUT_FILE = \"final_book_report.csv\"\n",
        "TOTAL_PAGES = 5  # Adjusted for testing; set to 50 for full site\n",
        "\n",
        "def get_bbc_keywords():\n",
        "    \"\"\"Crawls 30 news articles from BBC to extract top 30 keywords.\"\"\"\n",
        "    print(\"Step 1: Extracting keywords from 30 BBC News articles...\")\n",
        "    try:\n",
        "        response = requests.get(BBC_NEWS_URL, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        links = []\n",
        "        for a in soup.find_all('a', href=True):\n",
        "            href = a['href']\n",
        "            if \"/news/\" in href and not href.endswith(\"/news\"):\n",
        "                full_url = \"https://www.bbc.com\" + href if href.startswith('/') else href\n",
        "                if full_url not in links: links.append(full_url)\n",
        "            if len(links) >= 30: break\n",
        "\n",
        "        all_text = \"\"\n",
        "        for link in links:\n",
        "            try:\n",
        "                res = requests.get(link, timeout=5)\n",
        "                article_soup = BeautifulSoup(res.text, 'html.parser')\n",
        "                all_text += \" \" + article_soup.get_text().lower()\n",
        "            except: continue\n",
        "\n",
        "        words = re.findall(r'\\b[a-z]{5,}\\b', all_text) # Words with 5+ letters\n",
        "        stopwords = {'their', 'would', 'about', 'there', 'people', 'which', 'could'}\n",
        "        filtered = [w for w in words if w not in stopwords]\n",
        "        return [word for word, count in Counter(filtered).most_common(30)]\n",
        "    except Exception as e:\n",
        "        print(f\"BBC Error: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_real_book_metadata(title):\n",
        "    \"\"\"Fetches real Author, Popularity (Rating), and Review snippet from Google Books API.\"\"\"\n",
        "    try:\n",
        "        api_url = f\"https://www.googleapis.com/books/v1/volumes?q=intitle:{title.replace(' ', '+')}\"\n",
        "        data = requests.get(api_url, timeout=5).json()\n",
        "        if \"items\" in data:\n",
        "            volume = data[\"items\"][0][\"volumeInfo\"]\n",
        "            return {\n",
        "                \"author\": \", \".join(volume.get(\"authors\", [\"Unknown Author\"])),\n",
        "                \"popularity\": volume.get(\"averageRating\", \"N/A\"),\n",
        "                \"review\": volume.get(\"description\", \"No review found\")[:150] + \"...\"\n",
        "            }\n",
        "    except: pass\n",
        "    return {\"author\": \"Unknown\", \"popularity\": \"N/A\", \"review\": \"N/A\"}\n",
        "\n",
        "def scrape_and_process():\n",
        "    news_keywords = get_bbc_keywords()\n",
        "    print(f\"Top Keywords to match: {news_keywords}\\n\")\n",
        "\n",
        "    with open(OUTPUT_FILE, mode='w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Title\", \"Author\", \"Original Price\", \"Final Price\", \"Discount Status\", \"Popularity (0-5)\", \"Rating (Stars)\", \"Stock\", \"Review Snippet\"])\n",
        "\n",
        "        for page_num in range(1, TOTAL_PAGES + 1):\n",
        "            print(f\"Scraping Books Page {page_num}...\")\n",
        "            res = requests.get(PAGE_URL.format(page_num))\n",
        "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "            for pod in soup.find_all(\"article\", class_=\"product_pod\"):\n",
        "                title = pod.h3.a[\"title\"]\n",
        "\n",
        "                # 1. Get Real Metadata (Author/Popularity)\n",
        "                real_info = get_real_book_metadata(title)\n",
        "\n",
        "                # 2. Get Detail Page Info (Stock/Description)\n",
        "                detail_url = BASE_URL + pod.h3.a[\"href\"].replace(\"../../../\", \"\")\n",
        "                d_res = requests.get(detail_url)\n",
        "                d_soup = BeautifulSoup(d_res.text, \"html.parser\")\n",
        "\n",
        "                desc = d_soup.select_one('article.product_page > p').text.lower() if d_soup.select_one('article.product_page > p') else \"\"\n",
        "                price_raw = pod.find(\"p\", class_=\"price_color\").text\n",
        "                original_price = float(re.sub(r'[^\\d.]', '', price_raw))\n",
        "\n",
        "                stock_text = d_soup.find(\"p\", class_=\"instock availability\").get_text(strip=True)\n",
        "                stock_count = \"\".join(filter(str.isdigit, stock_text)) or \"0\"\n",
        "\n",
        "                star_map = {\"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4, \"Five\": 5}\n",
        "                star_rating = star_map.get(pod.p['class'][1], -1)\n",
        "\n",
        "                # 3. DISCOUNT LOGIC: Only apply if keyword matches description\n",
        "                match = any(word in desc for word in news_keywords)\n",
        "                if match:\n",
        "                    final_price = round(original_price * 0.5, 2)\n",
        "                    status = \"50% OFF (Match)\"\n",
        "                else:\n",
        "                    final_price = original_price\n",
        "                    status = \"No Match (Original Price)\"\n",
        "\n",
        "                writer.writerow([\n",
        "                    title, real_info['author'], original_price, final_price,\n",
        "                    status, real_info['popularity'], star_rating, stock_count, real_info['review']\n",
        "                ])\n",
        "                time.sleep(0.5) # Prevent API rate limits\n",
        "\n",
        "    print(f\"\\nCompleted! Check '{OUTPUT_FILE}' for the results.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    scrape_and_process()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwkt200Mm92E",
        "outputId": "02ca9cf7-bce2-4a42-dc94-b8e59cf0f51b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Extracting keywords from 30 BBC News articles...\n",
            "Top Keywords to match: ['after', 'ukraine', 'russian', 'latest', 'trump', 'russia', 'external', 'broadcasting', 'warwar', 'picturesbbc', 'killed', 'americamiddle', 'ukraineus', 'eastin', 'indepthbbc', 'ireland', 'military', 'years', 'other', 'president', 'british', 'putin', 'since', 'minister', 'police', 'attack', 'bondi', 'christmas', 'rights', 'university']\n",
            "\n",
            "Scraping Books Page 1...\n",
            "Scraping Books Page 2...\n",
            "Scraping Books Page 3...\n",
            "Scraping Books Page 4...\n",
            "Scraping Books Page 5...\n",
            "\n",
            "Completed! Check 'final_book_report.csv' for the results.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}